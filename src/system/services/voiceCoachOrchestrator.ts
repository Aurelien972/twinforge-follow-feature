/**
 * Voice Coach Orchestrator
 * Service central qui coordonne tous les composants du syst√®me vocal
 * G√®re la connexion, l'audio, les transcriptions et les r√©ponses
 */

import logger from '../../lib/utils/logger';
import { useUnifiedCoachStore } from '../store/unifiedCoachStore';
import type { VoiceState } from '../store/unifiedCoachStore';
import { openaiRealtimeService } from './openaiRealtimeService';
import { audioInputService } from './audioInputService';
import { audioOutputService } from './audioOutputService';

class VoiceCoachOrchestrator {
  private isInitialized = false;
  private audioBuffer: Float32Array[] = [];
  private isProcessingAudio = false;
  private silenceTimer: NodeJS.Timeout | null = null;
  private silenceDuration = 1500; // ms de silence avant d'envoyer
  private currentCoachMessage = ''; // Accumulation de la transcription du coach

  /**
   * Initialiser l'orchestrateur
   */
  async initialize(): Promise<void> {
    if (this.isInitialized) {
      logger.debug('VOICE_ORCHESTRATOR', 'Already initialized');
      return;
    }

    try {
      logger.info('VOICE_ORCHESTRATOR', 'Initializing voice coach orchestrator');

      // V√©rifier la configuration Supabase (n√©cessaire pour l'edge function)
      logger.debug('VOICE_ORCHESTRATOR', 'Checking Supabase configuration');
      const supabaseUrl = import.meta.env.VITE_SUPABASE_URL;
      const supabaseAnonKey = import.meta.env.VITE_SUPABASE_ANON_KEY;

      if (!supabaseUrl || !supabaseAnonKey) {
        logger.error('VOICE_ORCHESTRATOR', 'Supabase configuration missing');
        throw new Error('Supabase configuration missing');
      }

      logger.debug('VOICE_ORCHESTRATOR', 'Supabase configuration OK, setting up handlers');

      // Setup event handlers pour l'API Realtime
      logger.debug('VOICE_ORCHESTRATOR', 'Setting up Realtime handlers');
      this.setupRealtimeHandlers();

      // Setup event handlers pour l'audio input
      logger.debug('VOICE_ORCHESTRATOR', 'Setting up audio input handlers');
      this.setupAudioHandlers();

      // Initialiser le service de sortie audio
      logger.debug('VOICE_ORCHESTRATOR', 'Initializing audio output service');
      await audioOutputService.initialize(24000);

      this.isInitialized = true;

      logger.info('VOICE_ORCHESTRATOR', 'Voice coach orchestrator initialized successfully');
    } catch (error) {
      logger.error('VOICE_ORCHESTRATOR', 'Initialization failed', {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : undefined
      });
      throw error;
    }
  }

  /**
   * D√©marrer une session vocale
   */
  async startVoiceSession(mode: string): Promise<void> {
    const store = useUnifiedCoachStore.getState();

    try {
      logger.info('VOICE_ORCHESTRATOR', 'üöÄ Starting voice session', { mode });

      // V√©rifier l'√©tat actuel
      if (store.voiceState === 'listening' || store.voiceState === 'speaking') {
        logger.warn('VOICE_ORCHESTRATOR', 'Session already active');
        return;
      }

      // Passer en √©tat connecting
      logger.info('VOICE_ORCHESTRATOR', 'üì° Setting voice state to connecting');
      store.setVoiceState('connecting');

      // R√©cup√©rer la configuration du mode depuis unifiedCoachStore
      const modeConfig = store.modeConfigs[mode as any];

      if (!modeConfig) {
        throw new Error(`Invalid mode: ${mode}`);
      }

      // V√©rifier les permissions micro
      logger.info('VOICE_ORCHESTRATOR', 'üé§ Checking microphone permissions');
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        stream.getTracks().forEach(track => track.stop());
        logger.info('VOICE_ORCHESTRATOR', '‚úÖ Microphone permission granted');
      } catch (permError) {
        logger.error('VOICE_ORCHESTRATOR', '‚ùå Microphone permission denied', { error: permError });
        throw new Error('Microphone access required for voice sessions');
      }

      // Initialiser l'audio input si pas d√©j√† fait
      if (!audioInputService.initialized) {
        logger.info('VOICE_ORCHESTRATOR', 'üîä Initializing audio input service');
        await audioInputService.initialize({
          sampleRate: 24000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        });
        logger.info('VOICE_ORCHESTRATOR', '‚úÖ Audio input service initialized');
      }

      // Connexion √† l'API Realtime via notre edge function
      logger.info('VOICE_ORCHESTRATOR', 'üåê Connecting to Realtime API via edge function');

      await openaiRealtimeService.connect({
        model: 'gpt-4o-realtime-preview-2024-10-01',
        voice: 'alloy',
        temperature: 0.8,
        maxTokens: 4096
      });
      logger.info('VOICE_ORCHESTRATOR', '‚úÖ Connected to Realtime API');

      // Configurer la session avec le system prompt
      logger.info('VOICE_ORCHESTRATOR', '‚öôÔ∏è Configuring session with system prompt');
      openaiRealtimeService.configureSession(modeConfig.systemPrompt, mode as any);
      logger.info('VOICE_ORCHESTRATOR', '‚úÖ Session configured');

      // D√©marrer une conversation dans le store
      logger.info('VOICE_ORCHESTRATOR', 'üí¨ Starting conversation in store');
      store.startConversation(mode as any);

      // D√©marrer l'enregistrement audio
      logger.info('VOICE_ORCHESTRATOR', 'üéôÔ∏è Starting audio recording');
      audioInputService.startRecording();
      logger.info('VOICE_ORCHESTRATOR', '‚úÖ Audio recording started');

      // Passer en √©tat listening
      logger.info('VOICE_ORCHESTRATOR', 'üëÇ Setting voice state to listening');
      store.setVoiceState('listening');

      logger.info('VOICE_ORCHESTRATOR', '‚úÖ‚úÖ‚úÖ Voice session started successfully - STATE = LISTENING ‚úÖ‚úÖ‚úÖ');
    } catch (error) {
      logger.error('VOICE_ORCHESTRATOR', 'Failed to start voice session', {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : undefined
      });

      logger.error('VOICE_ORCHESTRATOR', '‚ùå CRITICAL: Voice session failed to start');
      store.setVoiceState('error');

      // Message d'erreur d√©taill√©
      let errorMessage = 'Failed to start voice session';
      if (error instanceof Error) {
        if (error.message.includes('permission')) {
          errorMessage = 'Microphone permission required';
        } else if (error.message.includes('connect') || error.message.includes('WebSocket')) {
          errorMessage = 'Unable to connect to voice service';
        } else {
          errorMessage = error.message;
        }
      }

      logger.error('VOICE_ORCHESTRATOR', 'üí• Error message for user', { errorMessage });
      store.setError(errorMessage);

      throw error;
    }
  }

  /**
   * Arr√™ter la session vocale
   */
  async stopVoiceSession(): Promise<void> {
    try {
      logger.info('VOICE_ORCHESTRATOR', 'Stopping voice session');

      // Arr√™ter l'enregistrement audio
      if (audioInputService.recording) {
        audioInputService.stopRecording();
      }

      // Envoyer le buffer audio final si n√©cessaire
      if (this.audioBuffer.length > 0) {
        await this.flushAudioBuffer();
      }

      // D√©connecter de l'API
      openaiRealtimeService.disconnect();

      // Arr√™ter la lecture audio
      audioOutputService.stop();

      // Nettoyer l'audio input
      audioInputService.cleanup();

      // Terminer la conversation
      const store = useUnifiedCoachStore.getState();
      store.endConversation();

      // Reset √©tat
      store.setVoiceState('idle');

      logger.info('VOICE_ORCHESTRATOR', 'Voice session stopped');
    } catch (error) {
      logger.error('VOICE_ORCHESTRATOR', 'Error stopping voice session', { error });
    }
  }

  /**
   * Setup des handlers pour l'API Realtime
   */
  private setupRealtimeHandlers(): void {
    // Handler pour les messages re√ßus
    openaiRealtimeService.onMessage((message) => {
      this.handleRealtimeMessage(message);
    });

    // Handler pour les erreurs
    openaiRealtimeService.onError((error) => {
      logger.error('VOICE_ORCHESTRATOR', '‚ùå Realtime API error', { error: error.message, stack: error.stack });

      const store = useUnifiedCoachStore.getState();
      store.setVoiceState('error');
      store.setError(error.message);
    });

    // Handler pour la connexion
    openaiRealtimeService.onConnect(() => {
      logger.info('VOICE_ORCHESTRATOR', '‚úÖ Realtime API WebSocket connected successfully');
    });

    // Handler pour la d√©connexion
    openaiRealtimeService.onDisconnect(() => {
      logger.info('VOICE_ORCHESTRATOR', 'üîå Realtime API disconnected');
    });
  }

  /**
   * Setup des handlers pour l'audio input
   */
  private setupAudioHandlers(): void {
    // Handler pour les donn√©es audio
    audioInputService.onAudioData((audioData) => {
      if (!this.isProcessingAudio) {
        this.handleAudioData(audioData);
      }
    });

    // Handler pour le niveau audio (pour visualisation)
    audioInputService.onAudioLevel((level) => {
      const store = useUnifiedCoachStore.getState();

      // Mettre √† jour la visualisation
      store.updateVisualization({
        volume: level.volume,
        isSpeaking: level.isSpeaking
      });

      // G√©rer la d√©tection automatique de silence (mode auto)
      if (level.isSpeaking) {
          // R√©initialiser le timer de silence
          if (this.silenceTimer) {
            clearTimeout(this.silenceTimer);
            this.silenceTimer = null;
          }
      } else if (store.voiceState === 'listening' && this.audioBuffer.length > 0) {
        // D√©marrer le timer de silence si pas d√©j√† actif
        if (!this.silenceTimer) {
          this.silenceTimer = setTimeout(() => {
            this.handleSilenceDetected();
          }, this.silenceDuration);
        }
      }
    });
  }

  /**
   * Traiter les donn√©es audio captur√©es
   */
  private handleAudioData(audioData: Float32Array): void {
    // Ajouter au buffer
    this.audioBuffer.push(new Float32Array(audioData));

    // Mettre √† jour les fr√©quences pour visualisation
    const frequencies = audioInputService.getFrequencyData();
    if (frequencies) {
      const store = useUnifiedCoachStore.getState();
      store.updateVisualization({
        frequencies: Array.from(frequencies.slice(0, 32))
      });
    }
  }

  /**
   * Silence d√©tect√© - envoyer l'audio
   */
  private async handleSilenceDetected(): Promise<void> {
    logger.debug('VOICE_ORCHESTRATOR', 'Silence detected, sending audio');

    this.silenceTimer = null;

    if (this.audioBuffer.length > 0) {
      await this.flushAudioBuffer();
    }
  }

  /**
   * Envoyer le buffer audio accumul√©
   */
  private async flushAudioBuffer(): Promise<void> {
    if (this.audioBuffer.length === 0) return;

    try {
      this.isProcessingAudio = true;

      // Concat√©ner tous les chunks
      const totalLength = this.audioBuffer.reduce((sum, chunk) => sum + chunk.length, 0);
      const concatenated = new Float32Array(totalLength);

      let offset = 0;
      for (const chunk of this.audioBuffer) {
        concatenated.set(chunk, offset);
        offset += chunk.length;
      }

      // Convertir en PCM16
      const pcm16 = this.float32ToPCM16(concatenated);

      // Envoyer √† l'API
      openaiRealtimeService.sendAudio(pcm16.buffer);
      openaiRealtimeService.commitAudioBuffer();

      // Vider le buffer
      this.audioBuffer = [];

      // Mettre √† jour l'√©tat
      const store = useUnifiedCoachStore.getState();
      logger.info('VOICE_ORCHESTRATOR', 'üîÑ Audio buffer flushed, setting state to processing');
      store.setVoiceState('processing');

      this.isProcessingAudio = false;
    } catch (error) {
      logger.error('VOICE_ORCHESTRATOR', 'Error flushing audio buffer', { error });
      this.isProcessingAudio = false;
    }
  }

  /**
   * Traiter les messages de l'API Realtime
   */
  private handleRealtimeMessage(message: any): void {
    const store = useUnifiedCoachStore.getState();

    logger.debug('VOICE_ORCHESTRATOR', 'üì® Received Realtime message', {
      type: message.type,
      hasContent: !!message.delta || !!message.transcript
    });

    switch (message.type) {
      // Transcription de l'utilisateur en cours (delta)
      case 'conversation.item.input_audio_transcription.delta':
        if (message.delta) {
          logger.info('VOICE_ORCHESTRATOR', 'üìù User transcription delta', { delta: message.delta });
          store.setCurrentTranscription(store.currentTranscription + message.delta);
        }
        break;

      // Transcription de l'utilisateur compl√®te
      case 'conversation.item.input_audio_transcription.completed':
        if (message.transcript) {
          logger.info('VOICE_ORCHESTRATOR', '‚úÖ User transcription completed', { transcript: message.transcript });
          store.setCurrentTranscription(message.transcript);

          // Ajouter le message utilisateur
          store.addMessage({
            role: 'user',
            content: message.transcript
          });

          // R√©initialiser la transcription courante
          store.setCurrentTranscription('');
        }
        break;

      // D√©but de r√©ponse du coach
      case 'response.audio.delta':
        // Changer l'√©tat en speaking d√®s le premier chunk audio
        if (store.voiceState !== 'speaking') {
          logger.info('VOICE_ORCHESTRATOR', 'üîä Coach audio started, setting state to speaking');
          store.setVoiceState('speaking');
          store.setSpeaking(true);
        }

        // Jouer l'audio re√ßu
        if (message.delta) {
          audioOutputService.addAudioChunk(message.delta);
        }
        break;

      // Transcription de la r√©ponse du coach (delta)
      case 'response.audio_transcript.delta':
        if (message.delta) {
          logger.info('VOICE_ORCHESTRATOR', 'üí¨ Coach transcript delta', { delta: message.delta.substring(0, 50) });
          this.currentCoachMessage += message.delta;

          // Mettre √† jour le dernier message ou en cr√©er un nouveau
          const messages = store.messages;
          const lastMessage = messages[messages.length - 1];

          if (lastMessage && lastMessage.role === 'coach') {
            // Mettre √† jour via le store (Zustand handle l'immutabilit√©)
            logger.debug('VOICE_ORCHESTRATOR', 'üîÑ Updating existing coach message');
            // On ne peut pas modifier directement, il faut recr√©er le tableau
            const updatedMessages = messages.slice(0, -1);
            store.clearMessages();
            updatedMessages.forEach(msg => store.addMessage(msg));
            store.addMessage({
              role: 'coach',
              content: this.currentCoachMessage
            });
          } else {
            // Cr√©er un nouveau message du coach
            logger.info('VOICE_ORCHESTRATOR', '‚ûï Creating new coach message');
            store.addMessage({
              role: 'coach',
              content: this.currentCoachMessage
            });
          }
        }
        break;

      // Transcription du coach compl√®te
      case 'response.audio_transcript.done':
        if (this.currentCoachMessage) {
          logger.info('VOICE_ORCHESTRATOR', '‚úÖ Coach transcript completed', {
            fullMessage: this.currentCoachMessage.substring(0, 100) + '...'
          });

          // R√©initialiser l'accumulation
          this.currentCoachMessage = '';
        }
        break;

      // Fin de r√©ponse audio
      case 'response.audio.done':
        logger.info('VOICE_ORCHESTRATOR', '‚úÖ Audio response completed');
        store.setSpeaking(false);
        break;

      // Fin de r√©ponse compl√®te
      case 'response.done':
        logger.info('VOICE_ORCHESTRATOR', '‚úÖ Response done, setting state back to listening');
        store.setVoiceState('listening');
        store.setProcessing(false);
        store.setSpeaking(false);
        break;

      // Erreur
      case 'error':
        logger.error('VOICE_ORCHESTRATOR', '‚ùå Realtime API error from server', {
          errorMessage: message.error?.message,
          errorType: message.error?.type,
          fullMessage: message
        });
        store.setVoiceState('error');
        store.setError(message.error?.message || 'Unknown error');
        audioOutputService.stop();
        break;

      // Session mise √† jour
      case 'session.updated':
        logger.info('VOICE_ORCHESTRATOR', '‚öôÔ∏è Session configuration updated');
        break;

      // D√©but de cr√©ation de r√©ponse
      case 'response.created':
        logger.info('VOICE_ORCHESTRATOR', 'üéØ Response creation started by server');
        store.setVoiceState('processing');
        store.setProcessing(true);
        break;

      default:
        logger.debug('VOICE_ORCHESTRATOR', '‚ùì Unhandled message type', { type: message.type });
    }
  }

  /**
   * Convertir Float32 en PCM16
   */
  private float32ToPCM16(float32Array: Float32Array): Int16Array {
    const pcm16 = new Int16Array(float32Array.length);

    for (let i = 0; i < float32Array.length; i++) {
      // Clamper entre -1 et 1
      const clamped = Math.max(-1, Math.min(1, float32Array[i]));
      // Convertir en Int16
      pcm16[i] = clamped < 0 ? clamped * 0x8000 : clamped * 0x7fff;
    }

    return pcm16;
  }

  /**
   * Nettoyer l'orchestrateur
   */
  cleanup(): void {
    if (this.silenceTimer) {
      clearTimeout(this.silenceTimer);
      this.silenceTimer = null;
    }

    audioOutputService.cleanup();
    this.audioBuffer = [];
    this.isProcessingAudio = false;
    this.currentCoachMessage = '';
    this.isInitialized = false;
  }

  /**
   * V√©rifier si l'orchestrateur est initialis√©
   */
  get initialized(): boolean {
    return this.isInitialized;
  }
}

// Export singleton
export const voiceCoachOrchestrator = new VoiceCoachOrchestrator();
